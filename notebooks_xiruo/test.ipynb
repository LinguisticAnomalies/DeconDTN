{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc2c551-8c28-41c4-b8f1-3d41d8195243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T21:07:36.265988Z",
     "iopub.status.busy": "2024-02-17T21:07:36.265248Z",
     "iopub.status.idle": "2024-02-17T21:07:40.736677Z",
     "shell.execute_reply": "2024-02-17T21:07:40.735425Z",
     "shell.execute_reply.started": "2024-02-17T21:07:36.265925Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import random\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "from transformers import DistilBertPreTrainedModel, PretrainedConfig, DistilBertModel\n",
    "from transformers import DistilBertConfig\n",
    "from transformers.models.distilbert.modeling_distilbert import SequenceClassifierOutput\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from typing import Union\n",
    "from typing import Tuple\n",
    "from scipy.special import softmax\n",
    "\n",
    "# train model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from utils import number_split, create_mix\n",
    "from data_process import load_wls_adress_AddDomain\n",
    "from process_SHAC import load_process_SHAC\n",
    "from custom_distance import KL\n",
    "\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eabf1cda-fe8f-4348-8f70-4a9f9b98f658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T21:33:43.999068Z",
     "iopub.status.busy": "2024-02-17T21:33:43.998312Z",
     "iopub.status.idle": "2024-02-17T21:34:23.621488Z",
     "shell.execute_reply": "2024-02-17T21:34:23.619718Z",
     "shell.execute_reply.started": "2024-02-17T21:33:43.999005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# (1) dynGen\n",
    "df_dynGen = pd.read_csv(\n",
    "    \"/bime-munin/xiruod/data/hateSpeech_Bulla2023/Dynamically-Generated-Hate-Speech-Dataset/Dynamically Generated Hate Dataset v0.2.3.csv\",\n",
    ")\n",
    "\n",
    "df_dynGen[\"label\"] = df_dynGen[\"label\"].map({\"hate\": \"hate\", \"nothate\": \"nothate\"})\n",
    "df_dynGen[\"dfSource\"] = \"dynGen\"\n",
    "df_dynGen[\"label_binary\"] = df_dynGen[\"label\"].map({\"hate\": 1, \"nothate\": 0})\n",
    "\n",
    "# (2)  wsf\n",
    "ls_allFiles = pathlib.Path(\n",
    "    \"/bime-munin/xiruod/data/hateSpeech_Bulla2023/hate-speech-dataset/all_files/\"\n",
    ").glob(\"*.txt\")\n",
    "\n",
    "ls_id = []\n",
    "ls_text = []\n",
    "\n",
    "for ifile in ls_allFiles:\n",
    "    ls_id.append(ifile.name.split(\".txt\")[0])\n",
    "    with open(ifile, \"r\") as f:\n",
    "        ls_text.append(f.read())\n",
    "\n",
    "df_wsf_raw = pd.DataFrame({\"file_id\": ls_id, \"text\": ls_text})\n",
    "\n",
    "df_wsf_annotation = pd.read_csv(\n",
    "    \"/bime-munin/xiruod/data/hateSpeech_Bulla2023/hate-speech-dataset/annotations_metadata.csv\"\n",
    ")\n",
    "\n",
    "df_wsf = df_wsf_raw.merge(df_wsf_annotation, on=\"file_id\", how=\"inner\")\n",
    "\n",
    "df_wsf = df_wsf[df_wsf[\"label\"].isin([\"hate\", \"noHate\"])].reset_index(drop=True)\n",
    "\n",
    "# df_wsf['label_binary'] = df_wsf['label'].map({\"hate\":1,\"noHate\":0})\n",
    "df_wsf[\"label\"] = df_wsf[\"label\"].map({\"hate\": \"hate\", \"noHate\": \"nothate\"})\n",
    "df_wsf[\"dfSource\"] = \"wsf\"\n",
    "df_wsf[\"label_binary\"] = df_wsf[\"label\"].map({\"hate\": 1, \"nothate\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "560100d6-f6bc-47b2-97ab-97e2cb40f05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T21:34:23.624124Z",
     "iopub.status.busy": "2024-02-17T21:34:23.623806Z",
     "iopub.status.idle": "2024-02-17T21:34:23.631014Z",
     "shell.execute_reply": "2024-02-17T21:34:23.630165Z",
     "shell.execute_reply.started": "2024-02-17T21:34:23.624093Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Hate Speech\n",
    "z_Categories = [\"dynGen\", \"wsf\"]  # the order here matters! Should match with df0, df1\n",
    "label = \"label_binary\"\n",
    "n_zCats = len(z_Categories)\n",
    "txt_col = \"text\"\n",
    "domain_col = \"dfSource\"\n",
    "df0 = df_dynGen\n",
    "df1 = df_wsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80755795-80bd-4360-b435-be32d1bc9d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T21:34:23.632800Z",
     "iopub.status.busy": "2024-02-17T21:34:23.632248Z",
     "iopub.status.idle": "2024-02-17T21:34:23.863012Z",
     "shell.execute_reply": "2024-02-17T21:34:23.862018Z",
     "shell.execute_reply.started": "2024-02-17T21:34:23.632768Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df0[\"labels\"] = le.fit_transform(df0[label])\n",
    "df1[\"labels\"] = le.fit_transform(df1[label])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6210d520-749c-4f23-8bef-ee37c8e685b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:05:49.778496Z",
     "iopub.status.busy": "2024-02-17T22:05:49.777799Z",
     "iopub.status.idle": "2024-02-17T22:06:23.963085Z",
     "shell.execute_reply": "2024-02-17T22:06:23.961991Z",
     "shell.execute_reply.started": "2024-02-17T22:05:49.778437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add labels and source to data for BERT\n",
    "for _ in [df0, df1]:\n",
    "    _[\"tokenized\"] = _[txt_col].apply(\n",
    "        lambda x: tokenizer(\n",
    "            x,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "        )\n",
    "    )\n",
    "    for i, row in _.iterrows():\n",
    "        row[\"tokenized\"].update(\n",
    "            {\"labels\": torch.tensor(int(row[\"labels\"]), dtype=torch.long)}\n",
    "        )\n",
    "        row[\"tokenized\"].update(\n",
    "            {\n",
    "                \"source\": torch.tensor(\n",
    "                    int(row[domain_col] == z_Categories[0]), dtype=torch.long\n",
    "                )\n",
    "            }\n",
    "        )  # NOTE: this only works for binary Category cases!!!\n",
    "        row[\"tokenized\"].update({\"input_ids\": row[\"tokenized\"][\"input_ids\"].squeeze()})\n",
    "        row[\"tokenized\"].update(\n",
    "            {\"attention_mask\": row[\"tokenized\"][\"attention_mask\"].squeeze()}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2908b126-7a12-4fac-b008-8cb1eb4e85c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:06:23.972170Z",
     "iopub.status.busy": "2024-02-17T22:06:23.971548Z",
     "iopub.status.idle": "2024-02-17T22:06:24.739387Z",
     "shell.execute_reply": "2024-02-17T22:06:24.738458Z",
     "shell.execute_reply.started": "2024-02-17T22:06:23.972132Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "##### Split\n",
    "# Hate Speech Detection: df_dynGen (0.55) vs df_wsf (0.11)\n",
    "n_test = 1000\n",
    "train_test_ratio = 4\n",
    "\n",
    "\n",
    "p_pos_train_z0_ls = [\n",
    "    0.2,\n",
    "    0.4,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.8,\n",
    "    0.9,\n",
    "]  # probability of training set examples drawn from site/domain z0 being positive\n",
    "p_pos_train_z1_ls = [\n",
    "    0.2,\n",
    "    0.4,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.8,\n",
    "    0.9,\n",
    "]  # probability of test set examples drawn from site/domain z1 being positive\n",
    "\n",
    "p_mix_z1_ls = [0.2, 0.4, 0.6, 0.8]  # = np.arange(0.1, 0.9, 0.05)\n",
    "\n",
    "# alpha_test_ls = np.arange(0, 10, 0.05)\n",
    "\n",
    "numvals = 1023\n",
    "base = 1.1\n",
    "alpha_test_ls = np.power(base, np.arange(numvals)) / np.power(base, numvals // 2)\n",
    "\n",
    "\n",
    "valid_full_settings = []\n",
    "for combination in itertools.product(\n",
    "    p_pos_train_z0_ls, p_pos_train_z1_ls, p_mix_z1_ls, alpha_test_ls\n",
    "):\n",
    "    number_setting = number_split(\n",
    "        p_pos_train_z0=combination[0],\n",
    "        p_pos_train_z1=combination[1],\n",
    "        p_mix_z1=combination[2],\n",
    "        alpha_test=combination[3],\n",
    "        train_test_ratio=train_test_ratio,\n",
    "        n_test=n_test,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    if number_setting is not None:\n",
    "        # if not (round(number_setting['mix_param_dict']['C_y'], 4) in [0.36, 0.44, 0.52]):\n",
    "        #     continue\n",
    "        if np.all([number_setting[k] >= 10 for k in list(number_setting.keys())[:-1]]):\n",
    "            valid_full_settings.append(number_setting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bd175ae3-cecc-4bb8-a767-68d077166bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:06:24.740749Z",
     "iopub.status.busy": "2024-02-17T22:06:24.740474Z",
     "iopub.status.idle": "2024-02-17T22:06:24.892503Z",
     "shell.execute_reply": "2024-02-17T22:06:24.891598Z",
     "shell.execute_reply.started": "2024-02-17T22:06:24.740724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b115da7fcfc648cfbba262fd79ec563c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_n_full_settings = []\n",
    "\n",
    "for c in tqdm(valid_full_settings):\n",
    "    # for c in test_settings:\n",
    "\n",
    "    c = c.copy()\n",
    "\n",
    "    # create train/test split according to stats\n",
    "    # dfs = create_mix(df0=df_wls_merge, df1=df_adress, target='label', setting= c, sample=False)\n",
    "    # dfs = create_mix(df0=df_shac_uw, df1=df_shac_mimic, target=label, setting= c, sample=False, seed=random.randint(0,1000))\n",
    "    dfs = create_mix(\n",
    "        df0=df0,\n",
    "        df1=df1,\n",
    "        target=label,\n",
    "        setting=c,\n",
    "        sample=False,\n",
    "        # seed=random.randint(0,1000),\n",
    "        seed=222,\n",
    "    )\n",
    "\n",
    "    if dfs is None:\n",
    "        continue\n",
    "\n",
    "    # #### TO DELETE: For results on Selected C_y ONLY!!!!!!!!!\n",
    "    # if round(c['mix_param_dict']['C_y'], 4) not in [0.36, 0.44, 0.52, 0.24, 0.54, 0.84]:\n",
    "    #     continue\n",
    "\n",
    "    c[\"run\"] = 3\n",
    "    valid_n_full_settings.append(c)\n",
    "\n",
    "    y_train = dfs[\"train\"][label]\n",
    "    y_test = dfs[\"test\"][label]\n",
    "\n",
    "    n_test = len(y_test)\n",
    "    \n",
    "    df_test = dfs[\"test\"].copy(deep=True)\n",
    " \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c664336c-cc62-4d6a-b037-d0347529ae95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:06:24.899887Z",
     "iopub.status.busy": "2024-02-17T22:06:24.899236Z",
     "iopub.status.idle": "2024-02-17T22:06:24.979713Z",
     "shell.execute_reply": "2024-02-17T22:06:24.978746Z",
     "shell.execute_reply.started": "2024-02-17T22:06:24.899859Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = set()\n",
    "for i, row in df_test.iterrows():\n",
    "    tmp.add(row['tokenized']['source'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f8b314d9-77f1-4a35-bb2c-5f84e7534206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:06:24.981121Z",
     "iopub.status.busy": "2024-02-17T22:06:24.980864Z",
     "iopub.status.idle": "2024-02-17T22:06:24.994022Z",
     "shell.execute_reply": "2024-02-17T22:06:24.992753Z",
     "shell.execute_reply.started": "2024-02-17T22:06:24.981099Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "106a571c-691c-4173-9a70-ea7977d12109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:06:24.996877Z",
     "iopub.status.busy": "2024-02-17T22:06:24.995910Z",
     "iopub.status.idle": "2024-02-17T22:06:25.008268Z",
     "shell.execute_reply": "2024-02-17T22:06:25.007306Z",
     "shell.execute_reply.started": "2024-02-17T22:06:24.996826Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                  11555.0\n",
       "acl.id                                                     acl20397\n",
       "X1                                                          11555.0\n",
       "text              Last nght I saw some japanese people walking o...\n",
       "label                                                          hate\n",
       "type                                                      animosity\n",
       "target                                                     asi.east\n",
       "level                                                      original\n",
       "split                                                           dev\n",
       "round.base                                                      2.0\n",
       "annotator                                                eLGzdD8Tvb\n",
       "round                                                            2a\n",
       "acl.id.matched                                             acl19953\n",
       "dfSource                                                     dynGen\n",
       "label_binary                                                      1\n",
       "labels                                                            1\n",
       "tokenized               [input_ids, attention_mask, labels, source]\n",
       "file_id                                                         NaN\n",
       "user_id                                                         NaN\n",
       "subforum_id                                                     NaN\n",
       "num_contexts                                                    NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "009aaa2e-d6c1-4268-b6e6-5e9ef14b2de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:06:25.009676Z",
     "iopub.status.busy": "2024-02-17T22:06:25.009324Z",
     "iopub.status.idle": "2024-02-17T22:06:25.029288Z",
     "shell.execute_reply": "2024-02-17T22:06:25.028410Z",
     "shell.execute_reply.started": "2024-02-17T22:06:25.009652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1045,  2052,  6235,  2032,  2004,  1037,  9346, 15667,  3054,\n",
       "        18150,  2007, 11238, 28583,  2015,  2725, 10124,  2923,  2731,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1), 'source': tensor(1)}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[12]['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd953b1e-3dcf-42a8-b6c2-0fe5c66db5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0de9ed8d-8748-4c96-86ee-4fe7a50aac91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:07:25.731787Z",
     "iopub.status.busy": "2024-02-17T22:07:25.731036Z",
     "iopub.status.idle": "2024-02-17T22:07:25.739985Z",
     "shell.execute_reply": "2024-02-17T22:07:25.738188Z",
     "shell.execute_reply.started": "2024-02-17T22:07:25.731725Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "08e31d19-e800-479a-b934-8b66b1e17fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:07:57.130938Z",
     "iopub.status.busy": "2024-02-17T22:07:57.130186Z",
     "iopub.status.idle": "2024-02-17T22:07:58.092427Z",
     "shell.execute_reply": "2024-02-17T22:07:58.091321Z",
     "shell.execute_reply.started": "2024-02-17T22:07:57.130877Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_test[f'tokenized_sr{i}'] = df_test.apply(lambda x: deepcopy(x[\"tokenized\"]), axis=1)\n",
    "    for _, row in df_test.iterrows():\n",
    "        row[f'tokenized_sr{i}']['source'] = torch.tensor(i, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0851b531-9c93-4b22-840b-b6e7f3344a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:02.300200Z",
     "iopub.status.busy": "2024-02-17T22:08:02.299751Z",
     "iopub.status.idle": "2024-02-17T22:08:02.311306Z",
     "shell.execute_reply": "2024-02-17T22:08:02.310590Z",
     "shell.execute_reply.started": "2024-02-17T22:08:02.300178Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140477605855184"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(df_test.iloc[1]['tokenized_sr0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "da1d6228-b631-4878-82ab-b174667f340f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:02.476079Z",
     "iopub.status.busy": "2024-02-17T22:08:02.475565Z",
     "iopub.status.idle": "2024-02-17T22:08:02.485569Z",
     "shell.execute_reply": "2024-02-17T22:08:02.484596Z",
     "shell.execute_reply.started": "2024-02-17T22:08:02.476043Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140477732757616"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(df_test.iloc[1]['tokenized_sr1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "08d57aeb-f80a-4f39-b2cc-951b48943014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:02.649975Z",
     "iopub.status.busy": "2024-02-17T22:08:02.649809Z",
     "iopub.status.idle": "2024-02-17T22:08:02.658174Z",
     "shell.execute_reply": "2024-02-17T22:08:02.656658Z",
     "shell.execute_reply.started": "2024-02-17T22:08:02.649962Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140478762427056"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(df_test.iloc[1]['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d39749f1-6e5f-426b-890f-e4a528309006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:03.145284Z",
     "iopub.status.busy": "2024-02-17T22:08:03.144915Z",
     "iopub.status.idle": "2024-02-17T22:08:03.158905Z",
     "shell.execute_reply": "2024-02-17T22:08:03.154362Z",
     "shell.execute_reply.started": "2024-02-17T22:08:03.145267Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[0]['tokenized_sr1']['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "935c13fb-8c42-4de7-9b18-20303b713def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:05.517960Z",
     "iopub.status.busy": "2024-02-17T22:08:05.517473Z",
     "iopub.status.idle": "2024-02-17T22:08:05.525735Z",
     "shell.execute_reply": "2024-02-17T22:08:05.524833Z",
     "shell.execute_reply.started": "2024-02-17T22:08:05.517932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[0]['tokenized_sr0']['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0419730b-41e9-4bd0-8463-09b70ee8f11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:07.459327Z",
     "iopub.status.busy": "2024-02-17T22:08:07.458935Z",
     "iopub.status.idle": "2024-02-17T22:08:07.466119Z",
     "shell.execute_reply": "2024-02-17T22:08:07.465268Z",
     "shell.execute_reply.started": "2024-02-17T22:08:07.459308Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[0]['tokenized']['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c40c9448-0b51-491c-8a14-61e328994781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:07.459327Z",
     "iopub.status.busy": "2024-02-17T22:08:07.458935Z",
     "iopub.status.idle": "2024-02-17T22:08:07.466119Z",
     "shell.execute_reply": "2024-02-17T22:08:07.465268Z",
     "shell.execute_reply.started": "2024-02-17T22:08:07.459308Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[0]['tokenized']['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3216587-d4d3-4b72-98e9-7253669f5ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "76e3357a-74da-4d8e-93a7-2b79c28c1b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T22:08:26.476096Z",
     "iopub.status.busy": "2024-02-17T22:08:26.475671Z",
     "iopub.status.idle": "2024-02-17T22:08:26.533583Z",
     "shell.execute_reply": "2024-02-17T22:08:26.532759Z",
     "shell.execute_reply.started": "2024-02-17T22:08:26.476075Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = set()\n",
    "for i, row in df_test.iterrows():\n",
    "    tmp.add(row['tokenized_sr4']['source'].tolist())\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238fee8-63ab-4597-ae9c-ce924069b8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
