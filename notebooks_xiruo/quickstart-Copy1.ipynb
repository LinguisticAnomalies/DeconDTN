{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Notebook\n",
    "\n",
    "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "The example uses the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
    "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
    "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:06:58.046339Z",
     "iopub.status.busy": "2023-09-06T19:06:58.046002Z",
     "iopub.status.idle": "2023-09-06T19:06:58.051674Z",
     "shell.execute_reply": "2023-09-06T19:06:58.050591Z",
     "shell.execute_reply.started": "2023-09-06T19:06:58.046295Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets\n",
    "# TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
    "# python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:06:58.054615Z",
     "iopub.status.busy": "2023-09-06T19:06:58.054239Z",
     "iopub.status.idle": "2023-09-06T19:06:58.067922Z",
     "shell.execute_reply": "2023-09-06T19:06:58.067003Z",
     "shell.execute_reply.started": "2023-09-06T19:06:58.054578Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:06:58.069897Z",
     "iopub.status.busy": "2023-09-06T19:06:58.069147Z",
     "iopub.status.idle": "2023-09-06T19:07:01.338990Z",
     "shell.execute_reply": "2023-09-06T19:07:01.337995Z",
     "shell.execute_reply.started": "2023-09-06T19:06:58.069861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If you are on a cluster, make sure you are on a CUDA machine!\n",
      "CUDA SETUP: CUDA runtime path found: /home/NETID/xiruod/anaconda3/envs/llama2/lib/libcudart.so\n",
      "CUDA SETUP: Loading binary /home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/NETID/xiruod/anaconda3/envs/llama2/lib/libcudart.so'), PosixPath('/home/NETID/xiruod/anaconda3/envs/llama2/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No GPU detected! Check your CUDA paths. Proceeding to load CPU-only library...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, LlamaConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.341269Z",
     "iopub.status.busy": "2023-09-06T19:07:01.340203Z",
     "iopub.status.idle": "2023-09-06T19:07:01.345309Z",
     "shell.execute_reply": "2023-09-06T19:07:01.344615Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.341244Z"
    }
   },
   "outputs": [],
   "source": [
    "class train_config:\n",
    "    def __init__(self):\n",
    "        self.quantization: bool = False\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.346219Z",
     "iopub.status.busy": "2023-09-06T19:07:01.346039Z",
     "iopub.status.idle": "2023-09-06T19:07:01.355482Z",
     "shell.execute_reply": "2023-09-06T19:07:01.354612Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.346203Z"
    }
   },
   "outputs": [],
   "source": [
    "globalconfig = train_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.357149Z",
     "iopub.status.busy": "2023-09-06T19:07:01.356667Z",
     "iopub.status.idle": "2023-09-06T19:07:01.367071Z",
     "shell.execute_reply": "2023-09-06T19:07:01.365576Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.357106Z"
    }
   },
   "outputs": [],
   "source": [
    "globalconfig.quantization = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.372499Z",
     "iopub.status.busy": "2023-09-06T19:07:01.371874Z",
     "iopub.status.idle": "2023-09-06T19:07:01.376476Z",
     "shell.execute_reply": "2023-09-06T19:07:01.375761Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.372448Z"
    }
   },
   "outputs": [],
   "source": [
    "globalconfig.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.377827Z",
     "iopub.status.busy": "2023-09-06T19:07:01.377406Z",
     "iopub.status.idle": "2023-09-06T19:07:01.753267Z",
     "shell.execute_reply": "2023-09-06T19:07:01.752284Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.377800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "globalconfig.model_id=\"/bime-munin/llama2_hf/llama-2-7b_hf/\"\n",
    "\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(model_id, \n",
    "#                                          load_in_8bit=False, \n",
    "#                                          device_map=\"cuda:0\", \n",
    "#                                          torch_dtype=torch.float16\n",
    "#                                         )\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(globalconfig.model_id)\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"}) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.755390Z",
     "iopub.status.busy": "2023-09-06T19:07:01.754852Z",
     "iopub.status.idle": "2023-09-06T19:07:01.761379Z",
     "shell.execute_reply": "2023-09-06T19:07:01.760444Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.755353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.763212Z",
     "iopub.status.busy": "2023-09-06T19:07:01.762826Z",
     "iopub.status.idle": "2023-09-06T19:07:01.772162Z",
     "shell.execute_reply": "2023-09-06T19:07:01.771249Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.763179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.773960Z",
     "iopub.status.busy": "2023-09-06T19:07:01.773374Z",
     "iopub.status.idle": "2023-09-06T19:07:01.782612Z",
     "shell.execute_reply": "2023-09-06T19:07:01.781611Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.773921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='/bime-munin/llama2_hf/llama-2-7b_hf/', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.784373Z",
     "iopub.status.busy": "2023-09-06T19:07:01.784037Z",
     "iopub.status.idle": "2023-09-06T19:07:01.791565Z",
     "shell.execute_reply": "2023-09-06T19:07:01.790732Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.784342Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_prompt = [\"Hello this is just a test and no conversations involved.\", \"this is just a test\"]\n",
    "# tokenizer(test_prompt, return_tensors='pt', max_length=20, padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "# token_inputs = tokenizer(test_prompt, return_tensors='pt', max_length=20, padding='max_length', truncation=True).to(\"cuda:0\")\n",
    "\n",
    "# tmp = model(**token_inputs, labels=torch.tensor([[2],[4]]).to(\"cuda:0\"))\n",
    "\n",
    "# tmp.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the preprocessed dataset\n",
    "\n",
    "We load and preprocess the samsum dataset which consists of curated pairs of dialogs and their summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T05:49:07.437879Z",
     "iopub.status.busy": "2023-09-06T05:49:07.437261Z",
     "iopub.status.idle": "2023-09-06T05:49:07.442352Z",
     "shell.execute_reply": "2023-09-06T05:49:07.441358Z",
     "shell.execute_reply.started": "2023-09-06T05:49:07.437835Z"
    }
   },
   "source": [
    "#### My Hate Speech Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:01.793492Z",
     "iopub.status.busy": "2023-09-06T19:07:01.792945Z",
     "iopub.status.idle": "2023-09-06T19:07:02.311730Z",
     "shell.execute_reply": "2023-09-06T19:07:02.310540Z",
     "shell.execute_reply.started": "2023-09-06T19:07:01.793459Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:02.313377Z",
     "iopub.status.busy": "2023-09-06T19:07:02.312993Z",
     "iopub.status.idle": "2023-09-06T19:07:02.773114Z",
     "shell.execute_reply": "2023-09-06T19:07:02.770981Z",
     "shell.execute_reply.started": "2023-09-06T19:07:02.313352Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dynGen = pd.read_csv(\"/bime-munin/xiruod/data/hateSpeech_Bulla2023/Dynamically-Generated-Hate-Speech-Dataset/Dynamically Generated Hate Dataset v0.2.3.csv\",)\n",
    "# df_dynGen['label_binary'] = df_dynGen['label'].map({\"hate\":1, \"nothate\":0})\n",
    "\n",
    "df_dynGen['label'] = df_dynGen['label'].map({\"hate\":\"hate\", \"nothate\":\"nothate\"})\n",
    "\n",
    "df_dynGen[\"dfSource\"] = \"dynGen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:02.775853Z",
     "iopub.status.busy": "2023-09-06T19:07:02.775332Z",
     "iopub.status.idle": "2023-09-06T19:07:52.824516Z",
     "shell.execute_reply": "2023-09-06T19:07:52.822738Z",
     "shell.execute_reply.started": "2023-09-06T19:07:02.775802Z"
    }
   },
   "outputs": [],
   "source": [
    "ls_allFiles = pathlib.Path(\"/bime-munin/xiruod/data/hateSpeech_Bulla2023/hate-speech-dataset/all_files/\").glob(\"*.txt\")\n",
    "\n",
    "ls_id = []\n",
    "ls_text = []\n",
    "\n",
    "for ifile in ls_allFiles:\n",
    "    ls_id.append(ifile.name.split(\".txt\")[0])\n",
    "    with open(ifile, \"r\") as f:\n",
    "        ls_text.append(f.read())\n",
    "\n",
    "df_wsf_raw = pd.DataFrame({\"file_id\":ls_id, \"text\":ls_text})\n",
    "\n",
    "df_wsf_annotation = pd.read_csv(\"/bime-munin/xiruod/data/hateSpeech_Bulla2023/hate-speech-dataset/annotations_metadata.csv\")\n",
    "\n",
    "df_wsf = df_wsf_raw.merge(df_wsf_annotation, on=\"file_id\", how=\"inner\")\n",
    "\n",
    "df_wsf = df_wsf[df_wsf['label'].isin(['hate','noHate'])].reset_index(drop=True)\n",
    "\n",
    "# df_wsf['label_binary'] = df_wsf['label'].map({\"hate\":1,\"noHate\":0})\n",
    "df_wsf['label'] = df_wsf['label'].map({\"hate\":\"hate\",\"noHate\":\"nothate\"})\n",
    "\n",
    "df_wsf[\"dfSource\"] = \"wsf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:52.827692Z",
     "iopub.status.busy": "2023-09-06T19:07:52.827207Z",
     "iopub.status.idle": "2023-09-06T19:07:53.166323Z",
     "shell.execute_reply": "2023-09-06T19:07:53.165335Z",
     "shell.execute_reply.started": "2023-09-06T19:07:52.827650Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:53.167992Z",
     "iopub.status.busy": "2023-09-06T19:07:53.167486Z",
     "iopub.status.idle": "2023-09-06T19:07:53.221571Z",
     "shell.execute_reply": "2023-09-06T19:07:53.220493Z",
     "shell.execute_reply.started": "2023-09-06T19:07:53.167965Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_dynGen, df_wsf]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:53.223176Z",
     "iopub.status.busy": "2023-09-06T19:07:53.222766Z",
     "iopub.status.idle": "2023-09-06T19:07:53.230643Z",
     "shell.execute_reply": "2023-09-06T19:07:53.229787Z",
     "shell.execute_reply.started": "2023-09-06T19:07:53.223151Z"
    }
   },
   "outputs": [],
   "source": [
    "df['label_binary'] = df['label'].map({\"hate\":1,\"nothate\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:53.232281Z",
     "iopub.status.busy": "2023-09-06T19:07:53.231827Z",
     "iopub.status.idle": "2023-09-06T19:07:53.275141Z",
     "shell.execute_reply": "2023-09-06T19:07:53.273621Z",
     "shell.execute_reply.started": "2023-09-06T19:07:53.232251Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df = datasets.Dataset.from_pandas(df[['text','dfSource','label_binary']].rename(columns={\"label_binary\":\"label\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:53.277081Z",
     "iopub.status.busy": "2023-09-06T19:07:53.276562Z",
     "iopub.status.idle": "2023-09-06T19:07:53.284214Z",
     "shell.execute_reply": "2023-09-06T19:07:53.283475Z",
     "shell.execute_reply.started": "2023-09-06T19:07:53.277045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'dalits are lowlives', 'dfSource': 'dynGen', 'label': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:53.285706Z",
     "iopub.status.busy": "2023-09-06T19:07:53.285282Z",
     "iopub.status.idle": "2023-09-06T19:07:53.293717Z",
     "shell.execute_reply": "2023-09-06T19:07:53.292795Z",
     "shell.execute_reply.started": "2023-09-06T19:07:53.285676Z"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {0:\"nothate\", 1:\"hate\"}\n",
    "label2id = {\"nothate\":0, \"hate\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:53.299771Z",
     "iopub.status.busy": "2023-09-06T19:07:53.299255Z",
     "iopub.status.idle": "2023-09-06T19:07:53.306221Z",
     "shell.execute_reply": "2023-09-06T19:07:53.305244Z",
     "shell.execute_reply.started": "2023-09-06T19:07:53.299735Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "\n",
    "    return  tokenizer(examples['text'], return_tensors='pt', max_length=1024, padding='max_length', truncation=True).to(globalconfig.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:07:53.308645Z",
     "iopub.status.busy": "2023-09-06T19:07:53.307946Z",
     "iopub.status.idle": "2023-09-06T19:08:35.781691Z",
     "shell.execute_reply": "2023-09-06T19:08:35.780870Z",
     "shell.execute_reply.started": "2023-09-06T19:07:53.308587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7a95c4eb2d40c9b08bbc884eb7d3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_df = dataset_df.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:08:35.783144Z",
     "iopub.status.busy": "2023-09-06T19:08:35.782773Z",
     "iopub.status.idle": "2023-09-06T19:08:35.787246Z",
     "shell.execute_reply": "2023-09-06T19:08:35.786485Z",
     "shell.execute_reply.started": "2023-09-06T19:08:35.783123Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_encodings = tokenizer(list(df['text']), return_tensors='pt', max_length=1024, padding='max_length', truncation=True).to(globalconfig.device)\n",
    "\n",
    "# train_labels = df['label']\n",
    "\n",
    "\n",
    "# class classificationDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "# train_dataset = classificationDataset(train_encodings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:08:35.788617Z",
     "iopub.status.busy": "2023-09-06T19:08:35.788248Z",
     "iopub.status.idle": "2023-09-06T19:12:22.143489Z",
     "shell.execute_reply": "2023-09-06T19:12:22.142458Z",
     "shell.execute_reply.started": "2023-09-06T19:08:35.788591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcd6870dae3419fb56d7f312529c65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /bime-munin/llama2_hf/llama-2-7b_hf/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(globalconfig.model_id, \n",
    "                                         load_in_8bit=globalconfig.quantization, \n",
    "                                         device_map=\"cuda:0\", \n",
    "                                         torch_dtype=torch.float16,\n",
    "                                                       num_labels = len(id2label), \n",
    "                                                       id2label=id2label,\n",
    "                                                       label2id=label2id,\n",
    "                                                       \n",
    "                                        )\n",
    "\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:12:22.146621Z",
     "iopub.status.busy": "2023-09-06T19:12:22.145557Z",
     "iopub.status.idle": "2023-09-06T19:12:38.290840Z",
     "shell.execute_reply": "2023-09-06T19:12:38.290014Z",
     "shell.execute_reply.started": "2023-09-06T19:12:22.146562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,210,688 || all params: 6,611,558,400 || trainable%: 0.06368677012669206\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    if globalconfig.quantization:\n",
    "        model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:26:55.051370Z",
     "iopub.status.busy": "2023-09-06T19:26:55.050828Z",
     "iopub.status.idle": "2023-09-06T19:26:55.060879Z",
     "shell.execute_reply": "2023-09-06T19:26:55.060023Z",
     "shell.execute_reply.started": "2023-09-06T19:26:55.051337Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"/bime-munin/xiruod/tmp/llama-output\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    # wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    wait, warmup, active, repeat = 10, 10, 100, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:27:01.966946Z",
     "iopub.status.busy": "2023-09-06T19:27:01.966442Z",
     "iopub.status.idle": "2023-09-06T19:27:01.975657Z",
     "shell.execute_reply": "2023-09-06T19:27:01.974161Z",
     "shell.execute_reply.started": "2023-09-06T19:27:01.966907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bime-munin/xiruod/tmp/llama-output'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T19:28:02.260595Z",
     "iopub.status.busy": "2023-09-06T19:28:02.259932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='439' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [439/500 10:52 < 01:31, 0.67 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1518.520600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2021.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>994.068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>920.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>884.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>506.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>578.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>626.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>354.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>317.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>189.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>524.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>483.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>583.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>226.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>524.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>423.977500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>169.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>284.916900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>262.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>266.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>285.581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>555.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>428.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>450.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>499.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>859.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>272.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>345.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>179.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>164.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=False,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    # max_steps=total_steps if enable_profiler else -1,\n",
    "    max_steps=500,\n",
    "\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_df,\n",
    "        data_collator=default_data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "ret_code = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T20:06:04.528366Z",
     "iopub.status.busy": "2023-09-06T20:06:04.527635Z",
     "iopub.status.idle": "2023-09-06T20:06:04.543421Z",
     "shell.execute_reply": "2023-09-06T20:06:04.542220Z",
     "shell.execute_reply.started": "2023-09-06T20:06:04.528316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T20:06:07.966537Z",
     "iopub.status.busy": "2023-09-06T20:06:07.965752Z",
     "iopub.status.idle": "2023-09-06T20:06:08.406912Z",
     "shell.execute_reply": "2023-09-06T20:06:08.405812Z",
     "shell.execute_reply.started": "2023-09-06T20:06:07.966488Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Try Fine-tuned Model\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-06T19:13:11.949353Z",
     "iopub.status.idle": "2023-09-06T19:13:11.950242Z",
     "shell.execute_reply": "2023-09-06T19:13:11.949971Z",
     "shell.execute_reply.started": "2023-09-06T19:13:11.949934Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:35:26.281684Z",
     "iopub.status.busy": "2023-09-06T22:35:26.281064Z",
     "iopub.status.idle": "2023-09-06T22:35:26.299011Z",
     "shell.execute_reply": "2023-09-06T22:35:26.298089Z",
     "shell.execute_reply.started": "2023-09-06T22:35:26.281632Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:35:26.301001Z",
     "iopub.status.busy": "2023-09-06T22:35:26.300585Z",
     "iopub.status.idle": "2023-09-06T22:35:29.839859Z",
     "shell.execute_reply": "2023-09-06T22:35:29.838552Z",
     "shell.execute_reply.started": "2023-09-06T22:35:26.300975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If you are on a cluster, make sure you are on a CUDA machine!\n",
      "CUDA SETUP: CUDA runtime path found: /home/NETID/xiruod/anaconda3/envs/llama2/lib/libcudart.so\n",
      "CUDA SETUP: Loading binary /home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/NETID/xiruod/anaconda3/envs/llama2/lib/libcudart.so'), PosixPath('/home/NETID/xiruod/anaconda3/envs/llama2/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/home/NETID/xiruod/anaconda3/envs/llama2/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No GPU detected! Check your CUDA paths. Proceeding to load CPU-only library...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, LlamaConfig\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:35:29.842323Z",
     "iopub.status.busy": "2023-09-06T22:35:29.841481Z",
     "iopub.status.idle": "2023-09-06T22:35:29.967699Z",
     "shell.execute_reply": "2023-09-06T22:35:29.966302Z",
     "shell.execute_reply.started": "2023-09-06T22:35:29.842285Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_model_id = \"/bime-munin/xiruod/tmp/llama-output/\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T23:18:20.575664Z",
     "iopub.status.busy": "2023-09-06T23:18:20.574925Z",
     "iopub.status.idle": "2023-09-06T23:18:20.582971Z",
     "shell.execute_reply": "2023-09-06T23:18:20.581938Z",
     "shell.execute_reply.started": "2023-09-06T23:18:20.575612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='/bime-munin/llama2_hf/llama-2-7b_hf/', revision=None, task_type='SEQ_CLS', inference_mode=True, r=8, target_modules=['q_proj', 'v_proj'], lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:35:29.970279Z",
     "iopub.status.busy": "2023-09-06T22:35:29.969451Z",
     "iopub.status.idle": "2023-09-06T22:39:32.762096Z",
     "shell.execute_reply": "2023-09-06T22:39:32.761102Z",
     "shell.execute_reply.started": "2023-09-06T22:35:29.970198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f546a22a30420098d2834862f24762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /bime-munin/llama2_hf/llama-2-7b_hf/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:39:32.764181Z",
     "iopub.status.busy": "2023-09-06T22:39:32.763748Z",
     "iopub.status.idle": "2023-09-06T22:39:32.773067Z",
     "shell.execute_reply": "2023-09-06T22:39:32.772215Z",
     "shell.execute_reply.started": "2023-09-06T22:39:32.764151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:39:32.774702Z",
     "iopub.status.busy": "2023-09-06T22:39:32.774269Z",
     "iopub.status.idle": "2023-09-06T22:39:49.111727Z",
     "shell.execute_reply": "2023-09-06T22:39:49.110419Z",
     "shell.execute_reply.started": "2023-09-06T22:39:32.774673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"}) \n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:39:49.113590Z",
     "iopub.status.busy": "2023-09-06T22:39:49.113155Z",
     "iopub.status.idle": "2023-09-06T22:39:49.120442Z",
     "shell.execute_reply": "2023-09-06T22:39:49.119573Z",
     "shell.execute_reply.started": "2023-09-06T22:39:49.113562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:39:49.121868Z",
     "iopub.status.busy": "2023-09-06T22:39:49.121493Z",
     "iopub.status.idle": "2023-09-06T22:39:49.131114Z",
     "shell.execute_reply": "2023-09-06T22:39:49.129959Z",
     "shell.execute_reply.started": "2023-09-06T22:39:49.121841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:39:49.133245Z",
     "iopub.status.busy": "2023-09-06T22:39:49.132781Z",
     "iopub.status.idle": "2023-09-06T22:39:49.149714Z",
     "shell.execute_reply": "2023-09-06T22:39:49.148962Z",
     "shell.execute_reply.started": "2023-09-06T22:39:49.133213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32001, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T22:39:49.151054Z",
     "iopub.status.busy": "2023-09-06T22:39:49.150699Z",
     "iopub.status.idle": "2023-09-06T22:39:49.168494Z",
     "shell.execute_reply": "2023-09-06T22:39:49.167286Z",
     "shell.execute_reply.started": "2023-09-06T22:39:49.151028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): LlamaForSequenceClassification(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32001, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(\n",
       "              in_features=4096, out_features=4096, bias=False\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(\n",
       "              in_features=4096, out_features=4096, bias=False\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (score): ModulesToSaveWrapper(\n",
       "      (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
       "      (modules_to_save): ModuleDict(\n",
       "        (default): Linear(in_features=4096, out_features=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
