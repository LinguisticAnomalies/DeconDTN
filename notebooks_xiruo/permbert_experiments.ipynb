{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertPreTrainedModel, PretrainedConfig, DistilBertModel\n",
    "from transformers.models.distilbert.modeling_distilbert import SequenceClassifierOutput\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from typing import Union\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PermDistilBertForSequenceClassification(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.num_sources = 2\n",
    "        self.config = config\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        # pemb[2] is the identity permutation\n",
    "        self.pemb = torch.stack([torch.randperm(config.dim) for i in range(2)] + [torch.arange(0,config.dim)])\n",
    "        self.pemb = torch.nn.Parameter(self.pemb, requires_grad=False)\n",
    "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_position_embeddings(self) -> nn.Embedding:\n",
    "        \"\"\"\n",
    "        Returns the position embeddings\n",
    "        \"\"\"\n",
    "        return self.distilbert.get_position_embeddings()\n",
    "\n",
    "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
    "        \"\"\"\n",
    "        Resizes position embeddings of the model if `new_num_position_embeddings != config.max_position_embeddings`.\n",
    "\n",
    "        Arguments:\n",
    "            new_num_position_embeddings (`int`):\n",
    "                The number of new position embedding matrix. If position embeddings are learned, increasing the size\n",
    "                will add newly initialized vectors at the end, whereas reducing the size will remove vectors from the\n",
    "                end. If position embeddings are not learned (*e.g.* sinusoidal position embeddings), increasing the\n",
    "                size will add correct vectors at the end following the position encoding algorithm, whereas reducing\n",
    "                the size will remove vectors from the end.\n",
    "        \"\"\"\n",
    "        self.distilbert.resize_position_embeddings(new_num_position_embeddings)\n",
    "\n",
    "   # @add_start_docstrings_to_model_forward(DISTILBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "   # @add_code_sample_docstrings(\n",
    "   #     checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "   #     output_type=SequenceClassifierOutput,\n",
    "   #     config_class=_CONFIG_FOR_DOC,\n",
    "   # )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        source: Optional[int] = None,\n",
    "    ) -> Union[SequenceClassifierOutput, Tuple[torch.Tensor, ...]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        \n",
    "        if source is None:\n",
    "            perm_ids = 2*np.ones(pooled_output.shape[0])\n",
    "        else:\n",
    "            perm_ids = np.asarray([source.cpu().numpy()]) #for i in range(pooled_output.shape[0])])\n",
    "        \n",
    "        perms = self.pemb[perm_ids]\n",
    "        sub_permuted = torch.gather(pooled_output,-1,perms)\n",
    "        \n",
    "        logits = self.classifier(sub_permuted)  # (bs, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + distilbert_output[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=30522,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    hidden_size=768,\n",
    "    intermediate_size=3072,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    pad_token_id=0,\n",
    "    eos_token_id=2,\n",
    "    bos_token_id=1,\n",
    "    sep_token_id=3,\n",
    "    cls_token_id=4,\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    use_cache=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PermDistilBertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231, 6251, 1012,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'source': tensor(2)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "inputs.update({\"source\":torch.tensor(2)})\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8280, -1.4565]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.eval()\n",
    "#model.cpu()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(\"This is a scary movie\", return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs.update({\"source\":torch.tensor(1)})\n",
    "\n",
    "    print(model(**inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"DeconDTN/dataToy/horror_family.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>tconst</th>\n",
       "      <th>id_w_tag</th>\n",
       "      <th>set</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8286</td>\n",
       "      <td>Larry Fessenden has been thrashed by most of t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>7</td>\n",
       "      <td>http://www.imdb.com/title/tt0275067/usercomments</td>\n",
       "      <td>tt0275067</td>\n",
       "      <td>pos_8286</td>\n",
       "      <td>train</td>\n",
       "      <td>movie</td>\n",
       "      <td>Wendigo</td>\n",
       "      <td>Wendigo</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>\\N</td>\n",
       "      <td>91</td>\n",
       "      <td>Horror,Mystery,Thriller</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8279</td>\n",
       "      <td>This film is more about how children make sens...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10</td>\n",
       "      <td>http://www.imdb.com/title/tt0275067/usercomments</td>\n",
       "      <td>tt0275067</td>\n",
       "      <td>pos_8279</td>\n",
       "      <td>train</td>\n",
       "      <td>movie</td>\n",
       "      <td>Wendigo</td>\n",
       "      <td>Wendigo</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>\\N</td>\n",
       "      <td>91</td>\n",
       "      <td>Horror,Mystery,Thriller</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text label  score  \\\n",
       "0  8286  Larry Fessenden has been thrashed by most of t...   pos      7   \n",
       "1  8279  This film is more about how children make sens...   pos     10   \n",
       "\n",
       "                                                url     tconst  id_w_tag  \\\n",
       "0  http://www.imdb.com/title/tt0275067/usercomments  tt0275067  pos_8286   \n",
       "1  http://www.imdb.com/title/tt0275067/usercomments  tt0275067  pos_8279   \n",
       "\n",
       "     set titleType primaryTitle originalTitle  isAdult  startYear endYear  \\\n",
       "0  train     movie      Wendigo       Wendigo        0       2001      \\N   \n",
       "1  train     movie      Wendigo       Wendigo        0       2001      \\N   \n",
       "\n",
       "  runtimeMinutes                   genres  domain  \n",
       "0             91  Horror,Mystery,Thriller  Horror  \n",
       "1             91  Horror,Mystery,Thriller  Horror  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#prepare df for training\n",
    "df = df.sample(frac=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#tokenize text\n",
    "df['bert'] = df['text'].apply(lambda x: tokenizer(x, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512))\n",
    "\n",
    "#encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>tconst</th>\n",
       "      <th>id_w_tag</th>\n",
       "      <th>set</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "      <th>domain</th>\n",
       "      <th>bert</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12389</td>\n",
       "      <td>Bored with the normal, run-of-the-mill staple ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>10</td>\n",
       "      <td>http://www.imdb.com/title/tt0076683/usercomments</td>\n",
       "      <td>tt0076683</td>\n",
       "      <td>pos_12389</td>\n",
       "      <td>train</td>\n",
       "      <td>movie</td>\n",
       "      <td>The Sentinel</td>\n",
       "      <td>The Sentinel</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>\\N</td>\n",
       "      <td>92</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Horror</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6283</td>\n",
       "      <td>What an incredible fall for Sean Ellis.&lt;br /&gt;&lt;...</td>\n",
       "      <td>neg</td>\n",
       "      <td>2</td>\n",
       "      <td>http://www.imdb.com/title/tt0906734/usercomments</td>\n",
       "      <td>tt0906734</td>\n",
       "      <td>neg_6283</td>\n",
       "      <td>train</td>\n",
       "      <td>movie</td>\n",
       "      <td>The Broken</td>\n",
       "      <td>The Broken</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>\\N</td>\n",
       "      <td>93</td>\n",
       "      <td>Drama,Horror,Thriller</td>\n",
       "      <td>Horror</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4821</td>\n",
       "      <td>Fiction film (it lists as based on a story tho...</td>\n",
       "      <td>neg</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.imdb.com/title/tt0078203/usercomments</td>\n",
       "      <td>tt0078203</td>\n",
       "      <td>neg_4821</td>\n",
       "      <td>train</td>\n",
       "      <td>movie</td>\n",
       "      <td>Sasquatch: The Legend of Bigfoot</td>\n",
       "      <td>Sasquatch: The Legend of Bigfoot</td>\n",
       "      <td>0</td>\n",
       "      <td>1976</td>\n",
       "      <td>\\N</td>\n",
       "      <td>95</td>\n",
       "      <td>Adventure,Horror,Mystery</td>\n",
       "      <td>Horror</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text label  score  \\\n",
       "0  12389  Bored with the normal, run-of-the-mill staple ...   pos     10   \n",
       "1   6283  What an incredible fall for Sean Ellis.<br /><...   neg      2   \n",
       "2   4821  Fiction film (it lists as based on a story tho...   neg      4   \n",
       "\n",
       "                                                url     tconst   id_w_tag  \\\n",
       "0  http://www.imdb.com/title/tt0076683/usercomments  tt0076683  pos_12389   \n",
       "1  http://www.imdb.com/title/tt0906734/usercomments  tt0906734   neg_6283   \n",
       "2  http://www.imdb.com/title/tt0078203/usercomments  tt0078203   neg_4821   \n",
       "\n",
       "     set titleType                      primaryTitle  \\\n",
       "0  train     movie                      The Sentinel   \n",
       "1  train     movie                        The Broken   \n",
       "2  train     movie  Sasquatch: The Legend of Bigfoot   \n",
       "\n",
       "                      originalTitle  isAdult  startYear endYear  \\\n",
       "0                      The Sentinel        0       1977      \\N   \n",
       "1                        The Broken        0       2008      \\N   \n",
       "2  Sasquatch: The Legend of Bigfoot        0       1976      \\N   \n",
       "\n",
       "  runtimeMinutes                    genres  domain  \\\n",
       "0             92                    Horror  Horror   \n",
       "1             93     Drama,Horror,Thriller  Horror   \n",
       "2             95  Adventure,Horror,Mystery  Horror   \n",
       "\n",
       "                          bert  labels  \n",
       "0  [input_ids, attention_mask]       1  \n",
       "1  [input_ids, attention_mask]       0  \n",
       "2  [input_ids, attention_mask]       0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#add labels and source to data for BERT\n",
    "for i,row in df.iterrows():\n",
    "    row[\"bert\"].update({\"labels\" : torch.tensor(int(row[\"labels\"]),dtype=torch.long)})\n",
    "    row[\"bert\"].update({\"source\" : torch.tensor(int(row.domain==\"Horror\"),dtype=torch.long)})\n",
    "    row[\"bert\"].update({\"input_ids\" : row[\"bert\"][\"input_ids\"].squeeze()})\n",
    "    row[\"bert\"].update({\"attention_mask\" : row[\"bert\"][\"attention_mask\"].squeeze()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=10,   # batch size for evaluation\n",
    "    warmup_steps=50,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = df.reset_index(drop=True)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x = 4\n",
    "print(train_df[\"bert\"].values[x][\"input_ids\"].shape)\n",
    "print(train_df[\"bert\"].values[x][\"attention_mask\"].shape)\n",
    "print(train_df[\"bert\"].values[x][\"labels\"])\n",
    "print(train_df[\"bert\"].values[x][\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)[\"bert\"].values[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_df[\"bert\"],         # training dataset\n",
    "    eval_dataset=test_df[\"bert\"],             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)[\"bert\"].values[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd2612495a143afb550eb3742c4c3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7553, 'learning_rate': 1e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5789, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.7095, 'learning_rate': 3e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6802, 'learning_rate': 4e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6497, 'learning_rate': 5e-05, 'epoch': 0.11}\n",
      "{'loss': 0.6957, 'learning_rate': 4.8746867167919805e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5588, 'learning_rate': 4.74937343358396e-05, 'epoch': 0.16}\n",
      "{'loss': 0.7008, 'learning_rate': 4.62406015037594e-05, 'epoch': 0.18}\n",
      "{'loss': 0.6151, 'learning_rate': 4.49874686716792e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6272, 'learning_rate': 4.3734335839599e-05, 'epoch': 0.22}\n",
      "{'loss': 0.5757, 'learning_rate': 4.24812030075188e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5232, 'learning_rate': 4.12280701754386e-05, 'epoch': 0.27}\n",
      "{'loss': 0.6498, 'learning_rate': 3.9974937343358395e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5833, 'learning_rate': 3.87218045112782e-05, 'epoch': 0.31}\n",
      "{'loss': 0.6592, 'learning_rate': 3.7468671679198e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6638, 'learning_rate': 3.6215538847117796e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6007, 'learning_rate': 3.49624060150376e-05, 'epoch': 0.38}\n",
      "{'loss': 0.7167, 'learning_rate': 3.3709273182957394e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5303, 'learning_rate': 3.24561403508772e-05, 'epoch': 0.42}\n",
      "{'loss': 0.6924, 'learning_rate': 3.120300751879699e-05, 'epoch': 0.45}\n",
      "{'loss': 0.5722, 'learning_rate': 2.9949874686716795e-05, 'epoch': 0.47}\n",
      "{'loss': 0.5789, 'learning_rate': 2.8696741854636594e-05, 'epoch': 0.49}\n",
      "{'loss': 0.6314, 'learning_rate': 2.7443609022556393e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5672, 'learning_rate': 2.6190476190476192e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6464, 'learning_rate': 2.493734335839599e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5882, 'learning_rate': 2.368421052631579e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5872, 'learning_rate': 2.243107769423559e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5724, 'learning_rate': 2.117794486215539e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5515, 'learning_rate': 1.9924812030075188e-05, 'epoch': 0.65}\n",
      "{'loss': 0.507, 'learning_rate': 1.8671679197994987e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6862, 'learning_rate': 1.7418546365914786e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5911, 'learning_rate': 1.6165413533834585e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5458, 'learning_rate': 1.4912280701754386e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5357, 'learning_rate': 1.3659147869674185e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4889, 'learning_rate': 1.2406015037593984e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4961, 'learning_rate': 1.1152882205513785e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4238, 'learning_rate': 9.899749373433584e-06, 'epoch': 0.82}\n",
      "{'loss': 0.4186, 'learning_rate': 8.646616541353383e-06, 'epoch': 0.85}\n",
      "{'loss': 0.4704, 'learning_rate': 7.393483709273183e-06, 'epoch': 0.87}\n",
      "{'loss': 0.3809, 'learning_rate': 6.140350877192982e-06, 'epoch': 0.89}\n",
      "{'loss': 0.4324, 'learning_rate': 4.887218045112782e-06, 'epoch': 0.91}\n",
      "{'loss': 0.4071, 'learning_rate': 3.6340852130325812e-06, 'epoch': 0.94}\n",
      "{'loss': 0.3826, 'learning_rate': 2.3809523809523808e-06, 'epoch': 0.96}\n",
      "{'loss': 0.3927, 'learning_rate': 1.1278195488721805e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082ecff510f0475f814196576af75258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3961559534072876, 'eval_accuracy': 0.8226381461675579, 'eval_f1': 0.7464968152866241, 'eval_precision': 0.7493606138107417, 'eval_recall': 0.7436548223350253, 'eval_runtime': 14.52, 'eval_samples_per_second': 77.273, 'eval_steps_per_second': 7.782, 'epoch': 1.0}\n",
      "{'train_runtime': 206.9574, 'train_samples_per_second': 21.676, 'train_steps_per_second': 2.17, 'train_loss': 0.5708138586949134, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=449, training_loss=0.5708138586949134, metrics={'train_runtime': 206.9574, 'train_samples_per_second': 21.676, 'train_steps_per_second': 2.17, 'train_loss': 0.5708138586949134, 'epoch': 1.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "mps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
